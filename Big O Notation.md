# Big O (How Code Slows as Data Grows)

**Big O notation** is a mathematical notation that describes the [limiting behavior](https://en.wikipedia.org/wiki/Asymptotic_analysis "Asymptotic analysis") of a [function](https://en.wikipedia.org/wiki/Function_(mathematics) "Function (mathematics)") when the [argument](https://en.wikipedia.org/wiki/Argument_of_a_function "Argument of a function") tends towards a particular value or infinity. Big O is a member of a [family of notations](https://en.wikipedia.org/wiki/Big_O_notation#Related_asymptotic_notations) invented by [Paul Bachmann](https://en.wikipedia.org/wiki/Paul_Gustav_Heinrich_Bachmann "Paul Gustav Heinrich Bachmann"), [Edmund Landau](https://en.wikipedia.org/wiki/Edmund_Landau) and others, collectively called **Bachmannâ€“Landau notation** or **asymptotic notation**.

It's used to [classify algorithms](https://en.wikipedia.org/wiki/Computational_complexity_theory "Computational complexity theory") according to how their run time or space requirements grow as the input size grows

---

Related: [Big O Notation by HackerRank](https://www.youtube.com/watch?v=v4cd1O4zkGw), [Introduction to Big O Notation and Time Complexity (Data Structures & Algorithms #7 by CS Dojo)](https://www.youtube.com/watch?v=D6xkbGLQesk)